# SportSync Project Blueprint (Planning v1)

Date: 2026-02-08  
Status: Draft  
Related:

1. `docs/AUTONOMY_LOOP_BLUEPRINT.md`
2. `docs/OPENCLAW_ORCHESTRATION_ARCHITECTURE.md`

## Scope Note

This document defines the product testbed (SportSync).  
The primary autonomous system design is captured in `docs/AUTONOMY_LOOP_BLUEPRINT.md` and is intentionally service-agnostic.

## 1) Project Goal

Build SportSync into a calm, minimal live sports companion that lets users:

1. Understand what matters now/today in under 15 seconds.
2. Drill into any event without navigating complex screens.
3. Ask natural-language questions and get source-grounded answers.
4. Benefit from continuous product improvements via sandboxed LLM agents.

## 2) Problem Statement

Current pain points:

1. Sports context is fragmented across many websites/apps.
2. During busy periods, it is hard to know which events matter most.
3. During live events, details (scores, tee times, stats) require manual digging.
4. Existing static update cadence is good for planning but weak for live context.

## 3) Product Principles

1. Minimal by default: show only what is needed at glance.
2. Progressive depth: details appear only on demand.
3. Facts first: deterministic data sources are source of truth.
4. LLM for meaning: summarize, prioritize, explain, compare.
5. Provider-agnostic AI: no lock-in to one model/vendor/runtime.
6. Safe autonomy: all coding agents are sandboxed and gate-checked.
7. Autonomous source growth: expand coverage through gated source onboarding.

## 4) Experience Model

Core flow:

`Glance -> Drill Down -> Ask`

### 4.1 Glance (Home)

Single screen with strict information hierarchy:

1. `Live Now` (in progress)
2. `Starting Soon` (next 2 hours)
3. `Later Today`
4. `Everything Else` (collapsed)

Rules:

1. Max 3 full-size focus cards visible.
2. Remaining events shown as compact grouped rows by sport.
3. One top summary sentence generated by LLM:
   - Example: "3 events need attention: Arsenal leads, Hovland climbs, Ruud under pressure."

### 4.2 Drill Down (Event Sheet)

Tap any event to open a unified detail sheet:

1. Snapshot: status, score/leaderboard/clock
2. What changed: key moments since last refresh
3. Deep details: stats, tee times, participants
4. Sources: direct raw links

### 4.3 Ask (Event Copilot)

Event-scoped chat by default:

1. Short answer first.
2. Evidence block second.
3. Optional "show raw source fields" third.

Quick prompts:

1. "What changed in last 10 minutes?"
2. "Who is trending right now?"
3. "When are the next tee times?"
4. "What matters most across all live events?"

## 5) Information Architecture

## Home Card Contract (summary)

```json
{
  "eventId": "string",
  "sport": "football|golf|tennis|f1|chess|esports",
  "state": "live|soon|later",
  "startTime": "ISO-8601",
  "statusLabel": "67' LIVE",
  "title": "Arsenal vs Man City",
  "keyNumber": "xG 1.8 - 0.9",
  "whyNow": "Arsenal regained lead and City switched to high press",
  "priorityScore": 87,
  "pinned": false
}
```

## Event Detail Contract (drilldown)

```json
{
  "eventId": "string",
  "snapshot": {},
  "recentChanges": [],
  "keyStats": [],
  "participants": [],
  "teeTimes": [],
  "sources": []
}
```

## 6) Technical Architecture

Three-layer model:

1. Deterministic data layer
   - Poll sports APIs and normalize into canonical event schema.
2. Intelligence layer
   - Rank/prioritize events.
   - Generate concise summaries and explanations.
   - Answer NL questions via tools over canonical data.
3. Delivery layer
   - Minimal frontend rendering `live-feed.json` + `event/{id}.json`.

### 6.1 Data Cadence

1. Static schedule feed (existing): every 6 hours.
2. Live feed: every 1-5 minutes (sport-dependent).
3. UI refresh: every 30-60 seconds while app open.

### 6.2 Priority Scoring (example)

`priority = eventStateWeight + userInterestWeight + volatilityWeight + recencyWeight`

Where:

1. eventStateWeight: live > soon > later
2. userInterestWeight: favorite sports/teams/players/pinned events
3. volatilityWeight: goals, lead changes, set breaks, round starts
4. recencyWeight: recent meaningful updates

## 7) LLM Provider Abstraction

Define one internal interface:

```ts
interface LlmEngine {
  summarizeLiveContext(input): Promise<Summary>;
  explainEventChange(input): Promise<Explanation>;
  answerEventQuestion(input): Promise<GroundedAnswer>;
}
```

Adapters:

1. `OpenAIEngine` (API key path)
2. `AnthropicEngine` (API key path)
3. `OpenClawEngine` (gateway/orchestrator path)

Selection:

1. Runtime provider setting in config.
2. Fallback chain if one provider fails.
3. Cost/latency telemetry per provider call.

## 8) Agent Roles

## 8.1 Live Ops Agents (product runtime)

1. Collector Agent
   - Fetch/normalize/update canonical events.
2. Prioritizer Agent
   - Compute ranking and focus list.
3. Narrator Agent
   - Produce top summary sentence + per-card "why now."
4. QA Agent
   - Validate outputs against schema and source references.
5. Publisher Agent
   - Publish `live-feed.json` and `event/{id}.json`.

## 8.2 Coding Autopilot Agent (engineering runtime)

1. Scope: small UX/perf/reliability improvements only.
2. Output: PRs only, never direct merge.
3. Hard gates: tests, schema validation, visual snapshots, perf checks.
4. Policy: no secrets/workflow/deploy edits unless explicitly approved.

## 9) Safety Model for Autonomous Coding

1. Ephemeral sandbox per run.
2. Limited filesystem write scope.
3. Network allowlist only.
4. Read-only secrets access or short-lived tokens.
5. Max diff size and changed-file allowlist.
6. Auto-rollback on regression in production metrics.

## 10) Implementation Phases

## Phase 0: Design + Contracts (now)

Deliverables:

1. Project blueprint and UX contract.
2. Canonical event schema and live-feed schema.
3. Provider-agnostic LLM interface design.
4. Source registry contract and trial/promotion criteria.

Exit criteria:

1. Agreed MVP scope.
2. Agreed "clean minimal UI" rules.

## Phase 1: Live Pulse MVP (no NL Q&A yet)

Deliverables:

1. `Live Now / Starting Soon / Later Today` home view.
2. Priority scoring + top 3 focus cards.
3. Compact grouped rows for overflow events.

Exit criteria:

1. Under-15-second comprehension target is met.

## Phase 2: Drill Down + Event Data Depth

Deliverables:

1. Event detail sheet with unified structure.
2. Consistent source-backed detail payload per event.

Exit criteria:

1. Users can answer detailed event questions without leaving app UI.

## Phase 3: Event Copilot (LLM)

Deliverables:

1. Event-scoped natural-language Q&A.
2. Short answer + evidence format.
3. Provider adapter support (OpenAI/Anthropic/OpenClaw).

Exit criteria:

1. Grounded answers with low hallucination risk.

## Phase 4: Safe Coding Autopilot

Deliverables:

1. Sandboxed agent pipeline for small iterative improvements.
2. PR gates + policy enforcement + audit trail.

Exit criteria:

1. At least 3 successful low-risk autonomous improvements merged safely.

## 11) Success Metrics

Product metrics:

1. Time to useful overview (target: <15s).
2. Daily "multi-site checking" reduction (self-reported and tracked sessions).
3. Live event engagement (open rate of live cards).
4. Event Copilot usefulness score.

Reliability metrics:

1. Data freshness lag by sport.
2. Provider error rates and fallback usage.
3. Hallucination/error reports for LLM answers.

Engineering metrics:

1. Autopilot PR pass/merge rate.
2. Regression rate post-merge.
3. Mean time to recover from bad changes.

## 12) Non-Goals (for MVP)

1. Full replacement of all specialist sports apps.
2. Fully autonomous direct-to-main code changes.
3. Complex multi-page navigation.
4. Provider-specific product logic in frontend.

## 13) Immediate Next Decisions

1. Define initial sports for live-first rollout (recommended: football, golf, tennis first).
2. Choose live refresh cadence per sport (1/2/5 min bands).
3. Lock first home card fields (strict minimal set).
4. Decide first provider for LLM adapter testing (interface remains generic).
5. Define source quality thresholds for autonomous source onboarding.
